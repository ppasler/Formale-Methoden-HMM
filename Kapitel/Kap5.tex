\chapter{Zusammenfassung}  \label{mainsec:result}
Wir haben in der vorgelegte Arbeit Grundlagen des Machine Learnings und der Wahrscheinlichekitsrechnung kennen gelernt. Weiterhin wurde mit Hilfe der Statistik die Markov Kette erläutert, die dann im Hidden Markov Model mündet und die Brücke zum Machine Learning schlägt. 

Der Vegleich des HMM mit anderen Ansätzen gestaltet sich schwierig, da es für alle Anwendungen und vorliegenden Daten, Experten gibt und diese oftmals nur durch ausprobieren eindeutig bestimmt werden können. 

Im folgenden Abschnitt werde ich ein kurzes Fazit ziehen.


\section{Fazit}
Das Hidden Markov Model eigent sich besonders für die Verarbeitung von sequentiellen Daten (Bspw. Sprache). Hier kann es, durch das Verweilen in einem internen Zustand, auch mit Verlängerungen eines Phonems in der gesprochenen Sprache sehr gut umgehen. Das mathematische Konzept hinter dem HMM ist sehr gut erforscht und formal definiert. Weiterhin sind Wahrscheinlichkeiten Größen, die ohne weiteres Verstanden werden können.
Dennoch benötigt man für das Verständnis von statistischen Verfahren, wie dem Hidden Markov Model, ein wenig Ahnung von Statistik (Siehe Abschnit \ref{sec:prop}).

Für die Spracherkennung, vor allem der sequentiellen Komponente, eigent sich das Hidden Markov Model hervorragend. Dennoch gibt es viele hybrid Ansätze, die in den einzelnen Schritten (Bswp. Trainingsphase) unterstützen.

Im Training müssen oftmals mehrere initiale Modelle erstellt und dann trainiert werden, um so das beste Ergebnis zu erhalten.   

HMM zeichnen sich durch hohe Performance aus und 