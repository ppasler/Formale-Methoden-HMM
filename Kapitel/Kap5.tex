\chapter{Zusammenfassung}  \label{mainsec:result}
Wir haben in der vorgelegte Arbeit Grundlagen des Machine Learnings und der Wahrscheinlichekitsrechnung kennen gelernt. Weiterhin wurde mit Hilfe der Statistik die Markov Kette erläutert, die dann im Hidden Markov Model mündet und die Brücke zum Machine Learning schlägt. 

Der Vegleich des HMM mit anderen Ansätzen gestaltet sich schwierig, da es für alle Anwendungen und vorliegenden Daten, Experten gibt und diese oftmals nur durch ausprobieren eindeutig bestimmt werden können. 

Im folgenden Abschnitt werde ich ein kurzes Fazit ziehen.


\section{Fazit}
Das Hidden Markov Model eigent sich besonders für die Verarbeitung von sequentiellen Daten (Bspw. Sprache). Hier kann es, durch das Verweilen in einem internen Zustand, auch mit Verlängerungen eines Phonems in der gesprochenen Sprache sehr gut umgehen. Das mathematische Konzept hinter dem HMM ist sehr gut erforscht und formal definiert. Weiterhin sind Wahrscheinlichkeiten Größen, die ohne weiteres verstanden werden können.
Dennoch benötigt man für das Verständnis von statistischen Verfahren, wie dem Hidden Markov Model, Kenntnisse von Statistik (Siehe Abschnit \ref{sec:prop}), was den Einstieg unter Umständen erschwert.

Für die Spracherkennung, vor allem der sequentiellen Komponente, eigent sich das Hidden Markov Model hervorragend. Dennoch exisitieren es viele hybride Ansätze \ref{mainsec:comp}, die sich die Vorteile von verschiedenen Algorithmen zu Nutze machen.
